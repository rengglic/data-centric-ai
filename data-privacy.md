# Data Privacy
_This area is a stub, you can help by improving it._

## Data Deletion
In recent years, the debate around data ownership in Machine Learning has intensified. Do individuals who contribute data to datasets have the "right to be forgotten"? What would it mean to fully delete someone's data in the context of machine learning, where data lives on inside models, even after the data has been removed from the training dataset? The EU has introduced legislation safeguarding the [right to be forgotten](https://gdpr.eu/right-to-be-forgotten/) and many public data resources include terms whereby individuals may ask for their data to be deleted. For example, in the [UK Biobank](https://www.ukbiobank.ac.uk/), one of the most valuable biomedical research databases, participants may request at any time that their data no longer be used. 

In modern machine learning, these requirements pose significant technical challenges. In most settings, retraining the model from scratch on the remaining data may be prohibitively expensive, especially since the requests for deletion may come at any time. Is it possible to efficiently remove datapoints from a trained model without retraining? [Ginart _et al._ ](https://papers.nips.cc/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf) propose a framework for studying this problem. They outline four principles that can guide the development of data deletion methods (_e.g._ modularity) and propose some methods for k-means clustering that are based on them. For more complex models, exact data deletion is challenging, so a number of approximate data deletion methods have been proposed. [Influence functions](https://arxiv.org/pdf/1703.04730.pdf) (also proposed by [Monari and Dreyfus](https://www.sciencedirect.com/science/article/abs/pii/S0925231200003258)) provide a natural method for approximate data deletion and rely only on the computation of the Hessian-gradient product. For linear and logistic models, we can sidstep this expensive computation, instead using the [projective residual update](https://arxiv.org/pdf/2002.10077.pdf), which is linear in the feature dimensionality. Evaluating the effectiveness of approximate data deletion methods poses yet another challenge. The [feature injection test](https://arxiv.org/pdf/2002.10077.pdf) is one approach. Others have tackled the data deletion problem under a slightly different guise, [Cauwenberghs and Poggio](https://papers.nips.cc/paper/2000/file/155fa09596c7e18e50b58eb7e0c6ccb4-Paper.pdf) refer to it as decremental learning and present an incremental procedure for training SVMs that are amenable to data deletion. [Tsai _et al._](https://dl.acm.org/doi/10.1145/2623330.2623661) propose a decremental learning method based on retraining with warm starts (which is also similar to the "machine unlearning" approach proposed by [Cao and Yang](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)).

It's worth noting that data deletion has applications beyond data ownershp and privacy. In the context of [data selection](data-selection.md), data deletion methods provide an efficient way to debug models by removing the influence of problematic datapoints. In [model evaluation](evaluation.md), they provide a means for efficiently performing leave-one-out cross validation. 
